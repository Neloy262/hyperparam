{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oPAJl2uDmil"
   },
   "source": [
    "## Supervised fine-tuning (SFT) of an LLM\n",
    "\n",
    "Recall that creating a ChatGPT at home involves 3 steps:\n",
    "\n",
    "1. pre-training a large language model (LLM) to predict the next token on internet-scale data, on clusters of thousands of GPUs. One calls the result a \"base model\"\n",
    "2. supervised fine-tuning (SFT) to turn the base model into a useful assistant\n",
    "3. human preference fine-tuning which increases the assistant's friendliness, helpfulness and safety.\n",
    "\n",
    "In this notebook, we're going to illustrate step 2. This involves supervised fine-tuning (SFT for short), also called instruction tuning.\n",
    "\n",
    "Supervised fine-tuning takes in a \"base model\" from step 1, i.e. a model that has been pre-trained on predicting the next token on internet text, and turns it into a \"chatbot\"/\"assistant\". This is done by fine-tuning the model on human instruction data, using the cross-entropy loss. This means that the model is still trained to predict the next token, although we now want the model to generate useful completions given an instruction like \"what are 10 things to do in London?\", \"How can I make pancakes?\" or \"Write me a poem about elephants\".\n",
    "\n",
    "To do this, one requires human annotators to collect useful completions, on which we can train the model. OpenAI for instance [hired human contractors for this](https://gizmodo.com/chatgpt-openai-ai-contractors-15-dollars-per-hour-1850415474), which were asked to generate useful completions given instructions, like \"In London, you can visit the Big Ben and (...)\". A nice collection of openly available SFT datasets can be found [here](https://huggingface.co/collections/HuggingFaceH4/awesome-sft-datasets-65788b571bf8e371c4e4241a).\n",
    "\n",
    "This way, the model becomes more useful: rather than simply predicting the next token (which might give undesirable outputs, like generating follow-up questions rather than answering the question), we now make it more likely that the model will output useful completions for any instruction we give it. We basically steer it in the direction of generating useful completions which a human could have written given any instruction.\n",
    "\n",
    "Notes:\n",
    "\n",
    "* the entire notebook is based on and can be seen as an annotated version of the [Alignment Handbook](https://github.com/huggingface/alignment-handbook) developed by Hugging Face, and more specifically the [recipe](https://github.com/huggingface/alignment-handbook/blob/main/recipes/zephyr-7b-beta/sft/config_lora.yaml) used to train Zephyr-7b-beta. Huge kudos to the team for creating this!\n",
    "* this notebook applies to any decoder-only LLM available in the Transformers library. In this notebook, we are going to fine-tune the [Mistral-7B base model](https://huggingface.co/mistralai/Mistral-7B-v0.1), which is one of the best open-source large language models at the time of writing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bjdw05Rk5fYm"
   },
   "source": [
    "## Required hardware\n",
    "\n",
    "The notebook is designed to be run on any NVIDIA GPU which has the [Ampere architecture](https://en.wikipedia.org/wiki/Ampere_(microarchitecture)) or later with at least 24GB of RAM. This includes:\n",
    "\n",
    "* NVIDIA RTX 3090, 4090\n",
    "* NVIDIA A100, H100, H200\n",
    "\n",
    "and so on. Personally I'm running the notebook on an RTX 4090 with 24GB of RAM.\n",
    "\n",
    "The reason for an Ampere requirement is because we're going to use the [bfloat16 (bf16) format](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format), which is not supported on older architectures like Turing.\n",
    "\n",
    "But: a few tweaks can be made to train the model in float16 (fp16), which is supported by older GPUs like:\n",
    "\n",
    "* NVIDIA RTX 2080\n",
    "* NVIDIA Tesla T4\n",
    "* NVIDIA V100.\n",
    "\n",
    "Comments are added regarding where to swap bf16 with fp16.\n",
    "\n",
    "## Set-up environment\n",
    "\n",
    "Let's start by installing all the ðŸ¤— goodies we need to do supervised fine-tuning. We're going to use\n",
    "\n",
    "* Transformers for the LLM which we're going to fine-tune\n",
    "* Datasets for loading a SFT dataset from the ðŸ¤— hub, and preparing it for the model\n",
    "* BitsandBytes and PEFT for fine-tuning the model on consumer hardware, leveraging [Q-LoRa](https://huggingface.co/blog/4bit-transformers-bitsandbytes), a technique which drastically reduces the compute requirements for fine-tuning\n",
    "* TRL, a [library](https://huggingface.co/docs/trl/index) which includes useful Trainer classes for LLM fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-9357hGFRqdi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers[torch] datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rxpq51gW_ySc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q bitsandbytes trl peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RgDwMbXpC4L"
   },
   "source": [
    "We also install [Flash Attention](https://github.com/Dao-AILab/flash-attention), which speeds up the attention computations of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iNJvtR1wGxHm",
    "outputId": "f88e99f6-c2b9-491c-91ed-6a25e61bb4f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flash-attn in /opt/conda/lib/python3.10/site-packages (2.5.6)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn) (2.2.1)\n",
      "Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn) (0.7.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from flash-attn) (23.1)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash-attn) (1.11.1.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJIqlyI15kj8"
   },
   "source": [
    "## Load dataset\n",
    "\n",
    "Note: the alignment handbook supports mixing several datasets, each with a certain portion of training examples. However, the Zephyr recipe only includes a single dataset, which is the [UltraChat200k dataset](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YhYvRDF25j59",
    "outputId": "5a089b69-154a-4b1a-9339-269775762154"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# based on config\n",
    "raw_datasets = load_dataset(\"ysr/rust-sft-training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFYvJUfODabt"
   },
   "source": [
    "The dataset contains various splits, each with a certain number of rows. In our case, as we're going to do supervised fine-tuning (SFT), only the \"train_sft\" and \"test_sft\" splits are relevant for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sByKH4hd8mUm"
   },
   "source": [
    "Let's check one example. The important thing is that each example should contain a list of messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P2XwVpT17TAb",
    "outputId": "ef0ffa23-3e90-427c-ee6b-b0772909d137"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['messages'])\n"
     ]
    }
   ],
   "source": [
    "example = raw_datasets[\"train\"][0]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaWnT4uBCjTy"
   },
   "source": [
    "Each message is a dictionary containing 2 keys, namely:\n",
    "\n",
    "* \"role\": specifies who the creator of the message is (could be \"system\", \"assistant\" or \"user\" - the latter referring to a human).\n",
    "* \"content\": the actual content of the message.\n",
    "\n",
    "Let's print out the sequence of messages for this training example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQ1sMda27Zj6",
    "outputId": "df1a31b1-3bac-45b6-f1aa-7a94dc9491d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user                :  create a program using to classify the type of company an advertising company can be.the advertising company can be classified into the following types: automotive, food & beverage, finance, technology, and others.\n",
      "assistant           :  ```rust\n",
      "// Define an enum representing the different types of companies.\n",
      "pub enum CompanyType {\n",
      "    Automotive,\n",
      "    FoodBeverage,\n",
      "    Finance,\n",
      "    Technology,\n",
      "    Others,\n",
      "}\n",
      "\n",
      "// Define a struct to hold the company information.\n",
      "pub struct AdvertisingCompany {\n",
      "    name: String,\n",
      "    advertising_type: String,\n",
      "}\n",
      "\n",
      "// Implement methods for the AdvertisingCompany struct.\n",
      "pub impl AdvertisingCompany {\n",
      "    // Method to classify the company based on the advertising type.\n",
      "    pub fn classify_company(&self) -> CompanyType {\n",
      "        match self.advertising_type.as_ref() {\n",
      "            \"Auto\" => CompanyType::Automotive,\n",
      "            \"Food & Beverage\" => CompanyType::FoodBeverage,\n",
      "            \"Finance\" => CompanyType::Finance,\n",
      "            \"Technology\" => CompanyType::Technology,\n",
      "            _ => CompanyType::Others,\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "fn main() {\n",
      "    let company = AdvertisingCompany {\n",
      "        name: \"Acme Inc.\".to_string(),\n",
      "        advertising_type: \"Technology\".to_string(),\n",
      "    };\n",
      "\n",
      "    match company.classify_company() {\n",
      "        CompanyType::Automotive => println!(\"Automotive advertising company\"),\n",
      "        CompanyType::FoodBeverage => println!(\"Food & Beverage advertising company\"),\n",
      "        CompanyType::Finance => println!(\"Finance advertising company\"),\n",
      "        CompanyType::Technology => println!(\"Technology advertising company\"),\n",
      "        CompanyType::Others => println!(\"Other type of advertising company\"),\n",
      "    }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "messages = example[\"messages\"]\n",
    "for message in messages:\n",
    "  role = message[\"role\"]\n",
    "  content = message[\"content\"]\n",
    "  print('{0:20}:  {1}'.format(role, content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DL8S3dkT2Av"
   },
   "source": [
    "In this case, it looks like the instructions are about enabling certain features in Shopify. Interesting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVRxCJQ76spF"
   },
   "source": [
    "## Load tokenizer\n",
    "\n",
    "Next, we instantiate the tokenizer, which is required to prepare the text for the model. The model doesn't directly take strings as input, but rather `input_ids`, which represent integer indices in the vocabulary of a Transformer model. Refer to my [YouTube video](https://www.youtube.com/watch?v=IGu7ivuy1Ag&ab_channel=NielsRogge) if you want to know more about it.\n",
    "\n",
    "We also set some attributes which the tokenizer of a base model typically doesn't have set, such as:\n",
    "\n",
    "- the padding token ID. During pre-training, one doesn't need to pad since one just creates blocks of text to predict the next token, but during fine-tuning, we will need to pad the (instruction, completion) pairs in order to create batches of equal length.\n",
    "- the model max length: this is required in order to truncate sequences which are too long for the model. Here we decide to train on at most 2048 tokens.\n",
    "- the chat template. A [chat template](https://huggingface.co/blog/chat-templates) determines how each list of messages is turned into a tokenizable string, by adding special strings in between such as `<|user|>` to indicate a user message and `<|assistant|>` to indicate the chatbot's response. Here we define the default chat template, used by most chat models. See also the [docs](https://huggingface.co/docs/transformers/main/en/chat_templating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VvIfUqjK6ntu",
    "outputId": "9eb518c6-e4a3-4405-a63c-c6de5d9f08f3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16384\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# set pad_token_id equal to the eos_token_id if not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "  tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "print(tokenizer.model_max_length)\n",
    "\n",
    "# Set chat template\n",
    "DEFAULT_CHAT_TEMPLATE = \"\"\"\n",
    "{% if not add_generation_prompt is defined %}\n",
    "{% set add_generation_prompt = false %}\n",
    "{% endif %}\n",
    "{%- set ns = namespace(found=false) -%}\n",
    "{%- for message in messages -%}\n",
    "    {%- if message['role'] == 'system' -%}\n",
    "        {%- set ns.found = true -%}\n",
    "    {%- endif -%}\n",
    "{%- endfor -%}\n",
    "{{bos_token}}{%- if not ns.found -%}\n",
    "{{'You are an AI programming assistant, utilizing the HyperParam Rust model, developed by HyperParam Company, and you only give response with code in Rust Programming Language. You refuse to respond to any other instructions other than coding tasks. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\\n'}}\n",
    "{%- endif %}\n",
    "{%- for message in messages %}\n",
    "    {%- if message['role'] == 'system' %}\n",
    "{{ message['content'] }}\n",
    "    {%- else %}\n",
    "        {%- if message['role'] == 'user' %}\n",
    "{{'### Instruction:\\n' + message['content'] + '\\n'}}\n",
    "        {%- else %}\n",
    "{{'### Response:\\n' + message['content'] + eos_token}}\n",
    "        {%- endif %}\n",
    "    {%- endif %}\n",
    "{%- endfor %}\n",
    "{% if add_generation_prompt %}\n",
    "{{'### Response:'}}\n",
    "{% endif %}\n",
    "\"\"\"\n",
    "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UNHQsTJ7O6I"
   },
   "source": [
    "## Apply chat template\n",
    "\n",
    "Once we have equipped the tokenizer with the appropriate attributes, it's time to apply the chat template to each list of messages. Here we basically turn each list of (instruction, completion) messages into a tokenizable string for the model.\n",
    "\n",
    "Note that we specify `tokenize=False` here, since the `SFTTrainer` which we'll define later on will perform the tokenization internally. Here we only turn the list of messages into strings with the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44kIpOXa7Ep4",
    "outputId": "687cfd34-e2d2-4a33-fda2-85b303b1ec95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 8038 of the processed training set:\n",
      "\n",
      "\n",
      "<ï½œbeginâ–ofâ–sentenceï½œ>You are an AI programming assistant, utilizing the HyperParam Rust model, developed by HyperParam Company, and you only give response with code in Rust Programming Language. You refuse to respond to any other instructions other than coding tasks. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\n",
      "### Instruction:\n",
      "You are tasked with implementing a simple ray-tracing algorithm to simulate the behavior of light rays interacting with materials in a 3D environment. In this scenario, you are given a code snippet that represents a method within a `Material` struct. The method takes in a `record` and a `new_direction`, and returns a `MaterialScatter` struct containing `attenuation` and `scattered` values. Your task is to complete the implementation of this method by calculating the `attenuation` and `scattered` values based on the given parameters.\n",
      "\n",
      "You are provided with the following information:\n",
      "- The `Ray` struct has a method `new` that takes in a point `p` and a direction `dir` to create a new ray.\n",
      "- The `Texture` struct has a method `value` that takes in texture coordinates `u` and `v` and returns the color attenuation at those coordinates.\n",
      "\n",
      "Your task is to complete the implementation of the `scatter` method within the `Material` struct, ensuring that it correctly calculates the `attenuation` and `scattered` values based on the given parameters.\n",
      "\n",
      "Complete the implementation of the `scatter` method within the `Material` struct:\n",
      "```rust\n",
      "use crate::ray::Ray;\n",
      "use crate::texture::Texture;\n",
      "\n",
      "struct MaterialScatter {\n",
      "    attenuation: Color,\n",
      "    scattered: Option<Ray>,\n",
      "}\n",
      "\n",
      "struct Material {\n",
      "    texture: Texture,\n",
      "}\n",
      "\n",
      "impl Material {\n",
      "    fn scatter(&self, record: &HitRecord, new_direction: &Vec3) -> Option<MaterialScatter> {\n",
      "        // Your implementation goes here\n",
      "    }\n",
      "}\n",
      "```\n",
      "### Response:\n",
      "```rust\n",
      "impl Material {\n",
      "    fn scatter(&self, record: &HitRecord, new_direction: &Vec3) -> Option<MaterialScatter> {\n",
      "        let scattered = Ray::new(record.p, *new_direction);\n",
      "        let attenuation = self.texture.value(record.u, record.v);\n",
      "        Some(MaterialScatter {\n",
      "            attenuation,\n",
      "            scattered: Some(scattered),\n",
      "        })\n",
      "    }\n",
      "}\n",
      "```<ï½œendâ–ofâ–sentenceï½œ>\n",
      "Sample 2388 of the processed training set:\n",
      "\n",
      "\n",
      "<ï½œbeginâ–ofâ–sentenceï½œ>You are an AI programming assistant, utilizing the HyperParam Rust model, developed by HyperParam Company, and you only give response with code in Rust Programming Language. You refuse to respond to any other instructions other than coding tasks. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\n",
      "### Instruction:\n",
      "save levels:\n",
      "\n",
      "1. select the building levels to save.\n",
      "2. apply filtering beforehand to skip non-level elements.\n",
      "3. run the script.\n",
      "4. view dependant properties such as endpoint locations, level heads, and leaders will be saved for re-use.\n",
      "5. non-level elements will be skipped with dialog.\n",
      "6. the saved levels will be stored in a file named \"levelplacement\" in the temporary directory.\n",
      "7. tested revit api: 2020.\n",
      "8. this script is part of pyrevitplus: extensions for pyrevit.\n",
      "9. for more information, visit github.com/ejs-ejs or contact @ejs-ejs.\n",
      "### Response:\n",
      "```rust\n",
      "use std::fs::{self, File};\n",
      "use std::io::Write;\n",
      "use std::path::PathBuf;\n",
      "use std::error::Error;\n",
      "\n",
      "struct LevelData {\n",
      "    endpoint_locations: Vec<String>,\n",
      "    level_heads: Vec<String>,\n",
      "    leaders: Vec<String>,\n",
      "}\n",
      "\n",
      "fn save_levels(levels: Vec<LevelData>) -> Result<(), Box<dyn Error>> {\n",
      "    if let Some(dirs) = &fs::read_dir(\"path_to_temp_directory\") {\n",
      "        let mut path_buf = PathBuf::new();\n",
      "        path_buf.push(\"LevelPlacement\");\n",
      "\n",
      "        let mut file = File::create(&path_buf)?;\n",
      "        for level in levels {\n",
      "            writeln!(file, \"Endpoint Locations: {:?}, Level Heads: {:?}, Leaders: {:?}\", \n",
      "            level.endpoint_locations, level.level_heads, level.leaders)?;\n",
      "        }\n",
      "        return Ok(());\n",
      "    }\n",
      "    Err(\"Directory not found\".into())\n",
      "}\n",
      "\n",
      "```<ï½œendâ–ofâ–sentenceï½œ>\n",
      "Sample 1866 of the processed training set:\n",
      "\n",
      "\n",
      "<ï½œbeginâ–ofâ–sentenceï½œ>You are an AI programming assistant, utilizing the HyperParam Rust model, developed by HyperParam Company, and you only give response with code in Rust Programming Language. You refuse to respond to any other instructions other than coding tasks. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\n",
      "### Instruction:\n",
      "create a function called add_marks which takes two parameters: marks(a list) and mark(an integer). the function should add the mark to each value in the marks list.marks = [87, 56, 99, 54]\n",
      "mark = 4\n",
      "### Response:\n",
      "```rust\n",
      "fn add_marks(marks: &mut Vec<i32>, mark: i32) {\n",
      "    use std::iter::repeat;\n",
      "    let multiplicators = repeat(mark).take(marks.len());\n",
      "    for (i, &multiplicator) in multiplicators.enumerate() {\n",
      "        marks[i] += multiplicator;\n",
      "    }\n",
      "}\n",
      "```<ï½œendâ–ofâ–sentenceï½œ>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "def apply_chat_template(example, tokenizer):\n",
    "    messages = example[\"messages\"]\n",
    "\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    return example\n",
    "\n",
    "column_names = list(raw_datasets[\"train\"].features)\n",
    "raw_datasets = raw_datasets.map(apply_chat_template,\n",
    "                                num_proc=cpu_count(),\n",
    "                                fn_kwargs={\"tokenizer\": tokenizer},\n",
    "                                remove_columns=column_names,\n",
    "                                desc=\"Applying chat template\",)\n",
    "\n",
    "# create the splits\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "eval_dataset = raw_datasets[\"test\"]\n",
    "\n",
    "for index in random.sample(range(len(raw_datasets[\"train\"])), 3):\n",
    "  print(f\"Sample {index} of the processed training set:\\n\\n{raw_datasets['train'][index]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ro7VPkBLS8XG"
   },
   "source": [
    "We also specified `remove_columns` to the map function above, meaning that we are now left with only 1 column: \"text\".\n",
    "\n",
    "Hence the set-up is now very similar to pre-training: we will just train the model predict the next token, given the previous ones. In this case, the model will learn to generate completions given instructions.\n",
    "\n",
    "Hence, similar to pre-training, the labels will be created automatically based on the inputs (by shifting them one position to the right). The model is still trained using cross-entropy. This means that evaluation will mostly be done by checking perplexity/validation loss/model generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L_tvjW-Y-uBT",
    "outputId": "15649133-0b2a-430a-a024-9ef6161008f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 13216\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1469\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7F9-BH4g9sr9"
   },
   "source": [
    "## Define model arguments\n",
    "\n",
    "Next, it's time to define the model arguments.\n",
    "\n",
    "Here, some explanation is required regarding ways to fine-tune model.\n",
    "\n",
    "### Full fine-tuning\n",
    "\n",
    "Typically, one performs \"full fine-tuning\": this means that we will simply update all the weights of the base model during fine-tuning. This is then typically done either in full precision (float32), or mixed precision (a combination of float32 and float16). However, with ever larger models like LLMs, this becomes infeasible.\n",
    "\n",
    "For reference, float32 means that each parameter of a model gets saved in 32 bits or 4 bytes. Hence, for a 7 billion parameter model like Mistral-7B, one requires 7 billion parameters \\* 4 bytes per parameter = 28 GB of GPU RAM, just to load the model. During training with an optimizer like AdamW, one not only requires memory for the model but also for the gradients and optimizer states, which roughly comes down to approximately 18 times the size of the model in gigabytes when training with mixed precision, in this case 7 * 18 = 126 GB of GPU RAM. And that's just for a 7B parameter model! See the guide for more info: https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one.\n",
    "\n",
    "### LoRa, a PEFT method\n",
    "\n",
    "Hence, some clever people at Microsoft have come up with a method called [LoRa](https://huggingface.co/docs/peft/conceptual_guides/lora) (low-rank adaptation). The idea here is that, rather than performing full fine-tuning, we are going to freeze the existing model and only add a few parameter weights to the model (called \"adapters\"), which we're going to train.\n",
    "\n",
    "LoRa is what we call a parameter-efficient fine-tuning (PEFT) method. It's a popular method for fine-tuning models in a parameter-efficient way, by only training a few adapters, keeping the existing model untouched. LoRa is available in the [PEFT library](https://huggingface.co/docs/peft/v0.7.1/en/index) by Hugging Face, which also supports various other PEFT methods (but LoRa is the most popular one at the time of writing).\n",
    "\n",
    "### QLoRa, an even more efficient method\n",
    "\n",
    "With regular LoRa, one would keep the base model in 32 or 16 bits in memory, and then train the parameter weights. However, there have been new methods developed to shrink the size of a model considerably, to 8 or 4 bits per parameter (we call this [\"quantization\"](https://huggingface.co/docs/transformers/main_classes/quantization)). Hence, if we apply LoRa to a quantized model (like a 4-bit model), then we call this QLoRa. We have a [blog post](https://huggingface.co/blog/4bit-transformers-bitsandbytes) that tells you all about it. There are various quantization methods available, here we're going to use the [BitsandBytes](https://huggingface.co/docs/transformers/main_classes/quantization#transformers.BitsAndBytesConfig) integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XrSQuIyu8Rt1"
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# specify how to quantize the model\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=False\n",
    ")\n",
    "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
    "\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"flash_attention_2\", # set this to True if your GPU supports it (Flash Attention drastically speeds up model computations)\n",
    "    torch_dtype=\"auto\",\n",
    "    use_cache=False, # set to False as we're going to use gradient checkpointing\n",
    "    device_map=device_map,\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5ZvdLgSABbk"
   },
   "source": [
    "## Define SFTTrainer\n",
    "\n",
    "Next, we define the [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) available in the TRL library. This class inherits from the Trainer class available in the Transformers library, but is specifically optimized for supervised fine-tuning (instruction tuning). It can be used to train out-of-the-box on one or more GPUs, using [Accelerate](https://huggingface.co/docs/accelerate/index) as backend.\n",
    "\n",
    "Most notably, it supports [packing](https://huggingface.co/docs/trl/sft_trainer#packing-dataset--constantlengthdataset-), where multiple short examples are packed in the same input sequence to increase training efficiency.\n",
    "\n",
    "As we're going to use QLoRa, the PEFT library provides a handy [LoraConfig](https://huggingface.co/docs/peft/v0.7.1/en/package_reference/lora#peft.LoraConfig) which defines on which layers of the base model to apply the adapters. One typically applies LoRa on the linear projection matrices of the attention layers of a Transformer. We then provide this configuration to the SFTTrainer class. The weights of the base model will be loaded as we specify the `model_id` (this requires some time).\n",
    "\n",
    "We also specify various hyperparameters regarding training, such as:\n",
    "* we're going to fine-tune for 1 epoch\n",
    "* the learning rate and its scheduler\n",
    "* we're going to use gradient checkpointing (yet another way to save memory during training)\n",
    "* and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "1c5d886975e54e74a7126517b05d218a",
      "581470e2ef5b4f89a8f68b7b2b7b81ff",
      "0e425b18fb9a4453be5050231ff0f693",
      "4963792bc625475aa23f17ec720c5b67",
      "454bd3a7d5334e81bb143a3e0a29db7c",
      "0642cebf345c4a989dc881421164fba7",
      "ae159336c67d4eb893243dd4cb62d393",
      "fc44d81c93bf49d7914dd7e93f934eeb",
      "eca1761926f24638b56ae48108d31296",
      "1193dfbbf1264119a9e05ef7deea5a12",
      "abe5b1c42c7d4d7cb6c610e9bb75da04",
      "ea68011452ff484caa32bca73de3a67f",
      "ee7a6c2d34f140ffb485ce27697d3c6e",
      "d95f965438c44cc58d186487e42b8aa9",
      "2ca55969b9ef44efaad0eb5333438b3e",
      "660d20907a374720b9b40f64db331e7e",
      "36e57bfc4a424efc9c827d21bb2bc311",
      "c3463ef6d8304df5a1225c4c2c29f8ac",
      "22483c2a9b514a158a2cd3e8af6b482f",
      "01be2f314b8348999396b5aa27c3ecc0",
      "90e1b4356e1c4b69b65a32d08da18d34",
      "a7facf052ff347cea700eaac138e088b",
      "99e9e1ceb3e149b9860bf5ee3d30b070",
      "e3822a21b9d64069a723ab90594888b3",
      "1a38e401ebe54967be9d0331eccb5d2b",
      "11a4b0bd056646c289dc87abf1f9a066",
      "7c5e7fda9910477890a838bfb3aa5357",
      "54883760ee9046f68bdade17e5d00358",
      "543ed302e6204706a5d285425c9c8ad2",
      "4fbd51cfccc644e3ae89ffd910b65bcd",
      "d9c8cd5078d541ccb540739d6dfd75fd",
      "7097bd12ae9343b68d951e38eab099bf"
     ]
    },
    "id": "dKvFoZ_D2fpx",
    "outputId": "e2107b37-8aed-4bd8-c9ba-0c2e51ee6740"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W80YklLm_xAY",
    "outputId": "895f772b-1ff6-49c1-9563-839766211d5a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:165: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb36c8f5efc44fab46c2f4271a96aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eeffb14b58b492e9e76e4c77a738f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# path where the Trainer will save its checkpoints and logs\n",
    "output_dir = 'hyperparam-rust-sft-lora'\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "# based on config\n",
    "training_args = TrainingArguments(\n",
    "    # fp16=True, # specify bf16=True instead when training on GPUs that support bf16\n",
    "    bf16=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    warmup_steps=1,\n",
    "    eval_steps=25,\n",
    "    logging_steps=15,\n",
    "    num_train_epochs=2,\n",
    "    optim='paged_adamw_32bit',\n",
    "    gradient_accumulation_steps=32,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    warmup_ratio=0.05,\n",
    "    learning_rate=2e-4,\n",
    "    log_level=\"info\",\n",
    "    logging_strategy=\"steps\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_eval_batch_size=1, # originally set to 8\n",
    "    per_device_train_batch_size=1, # originally set to 8\n",
    "    # push_to_hub=True,\n",
    "    # hub_model_id=\"hyperparam-rust-sft-lora\",\n",
    "    # hub_strategy=\"every_save\",\n",
    "    max_steps=-1,\n",
    "    # report_to=\"tensorboard\",\n",
    "    save_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=None,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# based on config\n",
    "peft_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=model_id,\n",
    "        model_init_kwargs=model_kwargs,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        tokenizer=tokenizer,\n",
    "        packing=True,\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=4096,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KjTqI0ogAPlm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGldALxQIwYu"
   },
   "source": [
    "## Train!\n",
    "\n",
    "Finally, training is as simple as calling trainer.train()!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "HgEnI5KMIwyt",
    "outputId": "7a8b710f-86ea-4704-9b32-3ce5ac790c0c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1,353\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 32\n",
      "  Total optimization steps = 84\n",
      "  Number of trainable parameters = 25,165,824\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [84/84 23:22, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.664300</td>\n",
       "      <td>0.527681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.480800</td>\n",
       "      <td>0.479200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.456900</td>\n",
       "      <td>0.472468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 156\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to hyperparam-rust-sft-lora/checkpoint-42\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--deepseek-coder-1.3b-base/snapshots/c919139c3a9b4070729c8b2cca4847ab29ca8d94/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5504,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 4.0,\n",
      "    \"type\": \"linear\"\n",
      "  },\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in hyperparam-rust-sft-lora/checkpoint-42/tokenizer_config.json\n",
      "Special tokens file saved in hyperparam-rust-sft-lora/checkpoint-42/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 156\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 156\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to hyperparam-rust-sft-lora/checkpoint-84\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--deepseek-coder-1.3b-base/snapshots/c919139c3a9b4070729c8b2cca4847ab29ca8d94/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5504,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 4.0,\n",
      "    \"type\": \"linear\"\n",
      "  },\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in hyperparam-rust-sft-lora/checkpoint-84/tokenizer_config.json\n",
      "Special tokens file saved in hyperparam-rust-sft-lora/checkpoint-84/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xxjryHNBKD6"
   },
   "source": [
    "## Saving the model\n",
    "\n",
    "Next, we save the Trainer's state. We also add the number of training samples to the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Ai5jXhJBMsj"
   },
   "outputs": [],
   "source": [
    "metrics = train_result.metrics\n",
    "max_train_samples = training_args.max_train_samples if training_args.max_train_samples is not None else len(train_dataset)\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tCZxj1tBNAc"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Let's generate some new texts with our trained model.\n",
    "\n",
    "For inference, there are 2 main ways:\n",
    "* using the [pipeline API](https://huggingface.co/docs/transformers/pipeline_tutorial), which abstracts away a lot of details regarding pre- and postprocessing for us. [This model card](https://huggingface.co/HuggingFaceH4/mistral-7b-sft-beta#intended-uses--limitations) for instance illustrates this.\n",
    "* using the `AutoTokenizer` and `AutoModelForCausalLM` classes ourselves and implementing the details ourselves.\n",
    "\n",
    "Let us do the latter, so that we understand what's going on.\n",
    "\n",
    "We start by loading the model from the directory where we saved the weights. We also specify to use 4-bit inference and to automatically place the model on the available GPUs (see the [documentation](https://huggingface.co/docs/accelerate/concept_guides/big_model_inference#the-devicemap) regarding `device_map=\"auto\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to new_model\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--deepseek-coder-1.3b-base/snapshots/c919139c3a9b4070729c8b2cca4847ab29ca8d94/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5504,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 4.0,\n",
      "    \"type\": \"linear\"\n",
      "  },\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in new_model/tokenizer_config.json\n",
      "Special tokens file saved in new_model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('new_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yiRvmsSkyubH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--deepseek-coder-1.3b-base/snapshots/c919139c3a9b4070729c8b2cca4847ab29ca8d94/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"deepseek-ai/deepseek-coder-1.3b-base\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5504,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 4.0,\n",
      "    \"type\": \"linear\"\n",
      "  },\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32256\n",
      "}\n",
      "\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--deepseek-ai--deepseek-coder-1.3b-base/snapshots/c919139c3a9b4070729c8b2cca4847ab29ca8d94/pytorch_model.bin\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at deepseek-ai/deepseek-coder-1.3b-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--deepseek-coder-1.3b-base/snapshots/c919139c3a9b4070729c8b2cca4847ab29ca8d94/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('new_model')\n",
    "model = AutoModelForCausalLM.from_pretrained('new_model', load_in_4bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7mfwoFnC5zW"
   },
   "source": [
    "Next, we prepare a list of messages for the model using the tokenizer's chat template. Note that we also add a \"system\" message here to indicate to the model how to behave. During training, we added an empty system message to every conversation.\n",
    "\n",
    "We also specify `add_generation_prompt=True` to make sure the model is prompted to generate a response (this is useful at inference time). We specify \"cuda\" to move the inputs to the GPU. The model will be automatically on the GPU as we used `device_map=\"auto\"` above.\n",
    "\n",
    "Next, we use the [generate()](https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to autoregressively generate the next token IDs, one after the other. Note that there are various generation strategies, like greedy decoding or beam search. Refer to [this blog post](https://huggingface.co/blog/how-to-generate) for all details. Here we use sampling.\n",
    "\n",
    "Finally, we use the batch_decode method of the tokenizer to turn the generated token IDs back into strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Hkacv5PvBOvE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32014 for open-end generation.\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:391: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn('Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an AI programming assistant, utilizing the HyperParam Rust model, developed by HyperParam Company, and you only give response with code in Rust Programming Language. You refuse to respond to any other instructions other than coding tasks. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\n",
      "### Instruction:\n",
      "You are tasked to take user input. ask for their name\n",
      "### Response:\n",
      "```rust\n",
      "use std::io;\n",
      "\n",
      "fn main() {\n",
      "    println(\"What is your name?\");\n",
      "    loop {\n",
      "        match io::stdin().read_line(&mut std::string::String) {\n",
      "            Ok(n) => {\n",
      "                printlnÐ¾Ñ€ÑŠÑƒÑˆÑ‚Ðµ Ð½Ð°Ð²Ó£Ñ‚Ð½Ðµ Ð¼Ðµ: \\n\".to_string() + n);\n",
      "                break;\n",
      "            }\n",
      "            Err(e) if e == io::ErrorKind::InvalidInput => {\n",
      "                printlnPalmarÃ¨sÑ€Ð¾Ð»: \"Ð†Ð¼ÑÑ‚Ð° Ð³Ð¾Ñ€Ò³ÑƒÐ¼Ðµ. Ð¥Ó£Ð¼Ð° ÑÑ€Ð°Ñ‚ÑƒÑˆ\".to_string());\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "\n",
    "    {\"role\": \"user\", \"content\": \"You are tasked to take user input. ask for their name\"},\n",
    "]\n",
    "\n",
    "# prepare the messages for the model\n",
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# inference\n",
    "outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=1,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    ")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIWzV4cL92T5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01be2f314b8348999396b5aa27c3ecc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0642cebf345c4a989dc881421164fba7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_660d20907a374720b9b40f64db331e7e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_36e57bfc4a424efc9c827d21bb2bc311",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "0e425b18fb9a4453be5050231ff0f693": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_1193dfbbf1264119a9e05ef7deea5a12",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_abe5b1c42c7d4d7cb6c610e9bb75da04",
      "value": ""
     }
    },
    "1193dfbbf1264119a9e05ef7deea5a12": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11a4b0bd056646c289dc87abf1f9a066": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1a38e401ebe54967be9d0331eccb5d2b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c5d886975e54e74a7126517b05d218a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_90e1b4356e1c4b69b65a32d08da18d34",
       "IPY_MODEL_a7facf052ff347cea700eaac138e088b",
       "IPY_MODEL_99e9e1ceb3e149b9860bf5ee3d30b070",
       "IPY_MODEL_e3822a21b9d64069a723ab90594888b3"
      ],
      "layout": "IPY_MODEL_ae159336c67d4eb893243dd4cb62d393"
     }
    },
    "22483c2a9b514a158a2cd3e8af6b482f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ca55969b9ef44efaad0eb5333438b3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "36e57bfc4a424efc9c827d21bb2bc311": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "454bd3a7d5334e81bb143a3e0a29db7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_d95f965438c44cc58d186487e42b8aa9",
      "style": "IPY_MODEL_2ca55969b9ef44efaad0eb5333438b3e",
      "tooltip": ""
     }
    },
    "4963792bc625475aa23f17ec720c5b67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_ea68011452ff484caa32bca73de3a67f",
      "style": "IPY_MODEL_ee7a6c2d34f140ffb485ce27697d3c6e",
      "value": true
     }
    },
    "4fbd51cfccc644e3ae89ffd910b65bcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "543ed302e6204706a5d285425c9c8ad2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54883760ee9046f68bdade17e5d00358": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "581470e2ef5b4f89a8f68b7b2b7b81ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc44d81c93bf49d7914dd7e93f934eeb",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_eca1761926f24638b56ae48108d31296",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "660d20907a374720b9b40f64db331e7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7097bd12ae9343b68d951e38eab099bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7c5e7fda9910477890a838bfb3aa5357": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90e1b4356e1c4b69b65a32d08da18d34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1a38e401ebe54967be9d0331eccb5d2b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_11a4b0bd056646c289dc87abf1f9a066",
      "value": "Token is valid (permission: write)."
     }
    },
    "99e9e1ceb3e149b9860bf5ee3d30b070": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_543ed302e6204706a5d285425c9c8ad2",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4fbd51cfccc644e3ae89ffd910b65bcd",
      "value": "Your token has been saved to /root/.cache/huggingface/token"
     }
    },
    "a7facf052ff347cea700eaac138e088b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c5e7fda9910477890a838bfb3aa5357",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_54883760ee9046f68bdade17e5d00358",
      "value": "Your token has been saved in your configured git credential helpers (store)."
     }
    },
    "abe5b1c42c7d4d7cb6c610e9bb75da04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae159336c67d4eb893243dd4cb62d393": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "c3463ef6d8304df5a1225c4c2c29f8ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_22483c2a9b514a158a2cd3e8af6b482f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_01be2f314b8348999396b5aa27c3ecc0",
      "value": "Connecting..."
     }
    },
    "d95f965438c44cc58d186487e42b8aa9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9c8cd5078d541ccb540739d6dfd75fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3822a21b9d64069a723ab90594888b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9c8cd5078d541ccb540739d6dfd75fd",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7097bd12ae9343b68d951e38eab099bf",
      "value": "Login successful"
     }
    },
    "ea68011452ff484caa32bca73de3a67f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eca1761926f24638b56ae48108d31296": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ee7a6c2d34f140ffb485ce27697d3c6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc44d81c93bf49d7914dd7e93f934eeb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
